---
title: "MY472 - Week 4: Seminar exercises in text analysis"
author: "Ryan Hübert"
date: "AT 2024"
output: html_document
---

**Attribution statement:** _The following teaching materials have been iteratively developed by current and former instructors, including: Friedrich Geiecke, Thomas Robinson and Ryan Hübert._

## Exercise 1: character encoding

Suppose that you are doing a project analysing news articles from around the world in different languages. Your collaborator sends you a news article saved in a plain-text file. Unfortunately, he does not know how it is encoded, or even which languages it is in. Figure out the file's character encoding and its language (Google or ChatGPT can help you identify the language if you cannot recognize it yourself). Then write a new copy on your computer with UTF-8 encoding and using a file name that indicates it is UTF-8.

```{r}
library(tidyverse)
my.file <- "data/news-article-1.txt"
raw <- read_file(my.file, locale = locale(encoding="___________"))
print(raw) # this is __________ [language]
my.utf.file <- "___________"
write_file(___________, ___________)
```


## Exercise 2: regex in R

Useful reference for string functions in R: <https://stringr.tidyverse.org/articles/stringr.html>

Useful reference for constructing regular expressions (regex) patterns: <https://github.com/rstudio/cheatsheets/blob/main/regex.pdf>


The file `uol.txt` in the course repo for this week (in the `data` folder) contains a list of 17 member institutions of the University of London. In R via `str_view()`, write down a regular expression which finds all postcodes. Note that in some of the postcodes there are white spaces between the two parts and in others not. The regular expression has to work with both.

```{r}
# library(tidyverse) # only load if you haven't already
text <- ________("data/uol.txt")
________(text, "________") 
```

Next, try to mute/delete the second part of each postcode. Add a capturing group to your regular expression with which you can select only the first part, i.e. the first 2-4 characters of the postcodes. Then use find & replace in your text editor or `str_replace_all()` in R, and replace all postcodes with only the information stored in capturing group 1. This deletes the second part of each postcode.

```{r}
txt <- ________(text, "________", "________")
print(txt)
```

Hint: In R, the way to reference a group in the replace is `"\\1"`.

## Exercise 3: basic text analysis

The `quanteda` package contains a lot of sample collections of documents you can use to practice your skills turning texts into quantitative datasets and then doing some basic text analysis. One of these contains the immigration section of the manifestos of nine UK political parties from 2010. It is called `data_char_ukimmig2010`, and it can be accessed after loading `quanteda`.

First, load `quanteda` and examine the collection of documents. Note: the collection of documents is stored as a named vector in R, so you can access its elements by index or name. The names indicate which political parties correspond to each document. The documents are the elements of the vector

```{r}
library(quanteda)
library(quanteda.textstats)

## 1. how many documents do we have?
________(data_char_ukimmig2010)
## 2. Can we look at the first 80 characters of each document?
for(document in ________){
  print(________(document, 1, 80)) ## There are a bunch of ways to do this!
}
## 3. That did not tell us which party produced which text. Now print the party names before the first 80 characters:
for(party in names(________)){
  print(paste0(party,": ",________(data_char_ukimmig2010[party], 1, 80))) ## There are a bunch of ways to do this!
}
```

Now, create a dfm using the following recipe. First, transform the collection of documents into a corpus, then remove punctuation and stopwords, then stem the words (`tokens_wordstem()`), and finally remove all words which are not at least contained in 2 documents. 

```{r}
my.dfm <- data_char_ukimmig2010 %>%                     # 1. start with the data
  corpus() %>%                                          
  tokens(___________ = TRUE, ___________ = TRUE) %>%    # 2. remove punctuation and numbers
  tokens_remove(___________) %>%                        # 3. remove stopwords (English)
  ___________() %>%                                     # 4. stem the tokens
  dfm() %>%
  dfm_trim(___________)                                 # 5. minimum word frequency = 2
my.dfm
```

Order the parties by the number of features (highest to lowest)

```{r}
rev(________(________(my.dfm)))
```

Identify the most commonly used words in the dfm by calculating which 10 words are the most common.

```{r}
________(my.dfm, ________)
```

Find any document features that refer work or any variation on the word 

```{r}
my.dfm %>% 
  ________(pattern = "________", ________) %>% 
  featnames()
```

Create a simple dictionary with two categories: economic issues, and legal issues. (This is a terrible dictionary for the purposes of studying these documents... you're just doing this to practice making dictionaries.)

```{r}
simple_dict <- ________(economic = c("work", "employ"), 
                    legal = c("asylum", "illeg", "deport"))
simple_dict <- ________(simple_dict)
simple_dict
```

How many times do words appear in the two categores in our dictionary within each document? What percentage of words in each document focuses on the two categories in our dictionary?

```{r}
________(my.dfm, dictionary = simple_dict) 
________(my.dfm, dictionary = simple_dict)/________(my.dfm)
```

## Optional

Imagine you have a specific document and would like to find those documents in a large set/database of documents that are most similar to it. This first seems like a daunting task, but could be useful both in academic research and private sector work (imagine a law firm that is looking for similar cases or researchers looking for similar articles). How could a computer programme achieve something like this? The trick of one possible approach is to combine your knowledge about text analysis with a bit of geometry and linear algebra. First, realise that every row in a dfm is actually a (transposed) vector of length/dimension K where K is the amount of features in the dfm. For a very brief introduction to vectors, see e.g. this excellent [video](https://youtu.be/fNk_zzaMoSs).

Let us assume for a moment that we only have three features/words in a dfm. Then every row/document is a 3 dimensional vector of counts and we can think of each document like a point in 3 dimensional space such as the room in which you are sitting. Axis 1 would denote the count of word 1 in the documents, axis 2 the count of word 2, axis 3 the count of word 3. Different vectors/documents would be in different parts of the space depending on which words they contain. (Normalised) vectors/documents of similar textual content should be in similar corners or areas of the room or space. With some help from mathematics we can in fact compute how similar or close these vectors or points in space are also quantitatively. The most frequently used approach to compute similarities between numerical vectors of word counts, also in high dimensional spaces with many different words, is [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity).

Using the following function, compute similarities between the text from the Conservatives and the ones of the other parties. Sort the resulting similarities to see which extracts are most similar to the one from the Conservative party. Note that the findings also depend on the assumptions made during cleaning and deleting terms, so these methods of course cannot yield definite answers but only some indication which has to be analysed further. For example, the documents of the different parties have very different lengths, e.g. the Coalition party's extract contains only 4 sentences. Cosine similarity is more robust against such differences in document lengths than other approaches, however, very different document lengths still make it difficult to compare documents.

```{r}
cosine_similarities <- function(document_name, dfm) {
  
  #
  # Compute similarities of one document to all other documents in dfm
  #
  # Inputs
  # document_name: Character corresponding to the target document/row in the quanteda dfm
  # dfm: Full quanteda dfm
  #
  # Output
  # Similarity of document to all others in dfm
  #
 
  dfm_row <- dfm[document_name,]
  dfm_row <- dfm_row/sqrt(rowSums(dfm_row^2))
  dfm <- dfm/sqrt(rowSums(dfm^2))
  
  similarities_output <- dfm%*%t(dfm_row)

  return(similarities_output)
  
}
```

```{r}
# Compute similarities
similarities <- cosine_similarities("Conservative", dfm)

# Store similarities in data frame
df_similarities <- similarities %>% as.matrix() %>% as.data.frame()
colnames(df_similarities) <- "similarity"

# Sort similarities
df_similarities %>%
  ___________(desc(similarity))
```


## References

- https://quanteda.io/articles/quickstart.html