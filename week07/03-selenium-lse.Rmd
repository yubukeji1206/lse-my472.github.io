---
title: "MY472 - Week 7: Using RSelenium to scrape programmes from the LSE website"
date: "Autumn Term 2024"
output: html_document
---

```{r setup, include=FALSE} 
# Because this script creates a marionette browser using RSelenium, I want to 
# be able to knit to HTML without evaluating each chunk.
knitr::opts_chunk$set(eval=FALSE)
```

**If you have not reviewed and/or completed the set up steps in `02-introduction-to-selenium.Rmd`, please do so before proceeding.**

The LSE website has as section where the user can enter a search term and a list of programmes is displayed potentially on several pages which are related to this term. This R markdown file develops a way to enter a search term, navigate through the resulting pages, and scrape all programmes/courses. The focus is thereby not to use the most efficient way to obtain this information, but rather to illustrate some key functionalities of Selenium when scraping complex web forms.

Loading the packages:

```{r}
library(RSelenium)
library(tidyverse)
library(netstat)
```

Launching the driver and browser. Recall that to ensure we don't get port conflicts we can use `netstat::free_port()`. 

```{r}
rD <- rsDriver(browser=c("firefox"), verbose = FALSE, port = netstat::free_port(random = TRUE)) # Switch off verbose, to minimize output/feedback
driver <- rD$client
```

Navigate to the respective website:

```{r}
url <- "https://www.lse.ac.uk/Programmes/Search-Courses"
driver$navigate(url)
```

First, let us create a list with the main XPath and class selectors which we use. We will need to know the location of the search tag box, the associated button (alternatively we can just use the enter key here), the box that displays which results are on the current page, and the button which leads on the page with the next set of results. The following values have been obtained with Inspect Element in Firefox:

```{r}
# Define some XPaths -- remember, use single and double quotes appropriately!
search_box <- '//*[@id="hero__search-input"]' 

## Find the portion of the page that says how many results there are, and how many are showing
# results_progress_box <- '/html/body/div[1]/div[1]/div/main/div/div[2]/div[1]/div/div[2]' # <- approach 1
results_progress_box <- '//div[@class="listing__info"]' # <- approach 2
```

Next, we want to build a helper function which later on e.g. allows us to know whether we are on the last page of a given search. For this we scrape the container which summarises the results on the current page:

```{r}
results_progress <- driver$findElement(using = "xpath",
                                       value = results_progress_box)
results_progress_text <- results_progress$getElementText()[[1]]
results_progress_text
```

Let us split this string into the following three pieces of information: 1. First result on this page (here: 1), 2. last result this page (here: 10), and the total number of results (here: 258).

```{r}
result_count <- str_extract_all(results_progress_text, "(?:^| )((?:[0-9]|[-])+)(?: |$)")[[1]]
result_count <- str_squish(result_count_str)
result_count <- c(str_split(result_count[1], "[-]")[[1]], result_count[2])
result_count <- as.numeric(result_count)
```

This works well. Next, let us write the same content into a function which can be reused and which returns a vector of length three with exactly this information:

```{r}
current_result_counts <- function() {

  results_progress <- driver$findElement(using = "xpath", value = results_progress_box)
  results_progress_text <- results_progress$getElementText()[[1]]
  result_count <- str_extract_all(results_progress_text, "(?:^| )((?:[0-9]|[-])+)(?: |$)")[[1]]
  result_count <- str_squish(result_count)
  result_count <- c(str_split(result_count[1], "[-]")[[1]], result_count[2])
  result_count <- as.numeric(result_count)
  
  return(result_count)
}

```

```{r}
current_result_counts()
```

We will use this helper function to determine whether we are on the last page of results. You can probably already guess what we are going to do: We will know we are on the last page if the second element is equal to the third element in this vector. Next, let us continue to defining some further functions that we will need:

```{r}
# first new function: move to next page of results
next_page <- function() {
  
  next_page_button <- driver$findElement(using = "xpath", value = '//span[text()="Next"]') # Hint: use text of button!
  next_page_button$clickElement()

}

# second new function: search for a term
search_for <- function(term) {
  
  # Find the search field, clear it, and enter the new search term, e.g. "data science"
  search_field <- driver$findElement(using = "xpath", value = selector_list$search_box)
  search_field$clearElement()
  search_field$sendKeysToElement(list(term))
  
  # Wait for one second and then press the enter key
  Sys.sleep(1)
  search_field$sendKeysToElement(list(key = "enter"))
  
}
```

Now, let us try out these functions:

```{r}
next_page()
```

```{r}
search_for("data science")
```

(Note that for moving to the next page it can be more efficient to figure out the URL structure of subsequent pages and navigate to these URLs directly rather than clicking on a next page button (as e.g. in the example of unstructured data from the lab last week). We're obviously using that information anyway, so it might be a better approach here. But to further highlight the functionality of Selenium in this file, we choose the approach to click instead. As a practice exercise: Instead of interactively clicking on the next button, can you build a scraper that iterates over the pages of search results by navigating to each page of results using the url?)

The last remaining question is how we identify the programme names on the page. One approach is to use XPaths and a loop for this. Let us copy the XPath of the first two programmes names in our search results:

/html/body/div[1]/div[1]/div/main/div/div[2]/div[1]/div/div[3]/div[1]/div/div/h3/a
/html/body/div[1]/div[1]/div/main/div/div[2]/div[1]/div/div[3]/div[2]/div/div/h3/a

Here the knowledge of XPath is helpful. The second programme element seems to be the second child of the same division. Hence we can just increment this integer to scrape the relevant elements on the page.

Now we can write the main scraping function:

```{r}
scrape_programmes <- function(term) {

  # Create a vector that will store results, initialise overall item counts, and
  # define a logical value that is set to true when we are on the last page
  all_programmes <- c()
  item_count <- 1
  last_page_flag <- FALSE
  
  # First, navigate the browser to the main programmes page
  url <- "https://www.lse.ac.uk/Programmes/Search-Courses"
  driver$navigate(url)
  Sys.sleep(2) # Notice again, we always embed delays after we make requests to webservers. This is good etiquette!
  
  # Next, enter search term
  search_for(term)
  Sys.sleep(4)

  # While we are not on the final page continue this loop
  while (last_page_flag == FALSE) {
    
    # Obtain the information about which elements are displayed on this page
    current_result_counts_vector <- current_result_counts()
    first_result_this_page <- current_result_counts_vector[1]
    last_result_this_page <- current_result_counts_vector[2]
    total_results <- current_result_counts_vector[3]

    # Compute the amount of items on this page
    programmes_on_this_page <- last_result_this_page - first_result_this_page + 1
    
    # Loop over the programmes on this page
    for (programme_int in 1:programmes_on_this_page) {
      
      # Create the XPath character of the current programme
      current_programe_xpath <- sprintf("/html/body/div[1]/div[1]/div/main/div/div[2]/div[1]/div/div[3]/div[%g]/div/div/h3/a", programme_int)
      
      # Find the element on the website and transform it to text directly
      current_programme_text <- driver$findElement(using = "xpath",
                                                   value = current_programe_xpath)$getElementText()[[1]]
      
      # Add the outcome to the vector
      all_programmes <- c(all_programmes, current_programme_text)
      
      # Increment the overall item count for the next element which is stored
      # in the list
      item_count <- item_count + 1
      
    }
    
    # If we are on the last page, set the flag to TRUE and thereby leave the
    # while loop afterwards
    if (last_result_this_page == total_results) {
      
      last_page_flag = TRUE
      
    # Otherwise, click on the next-page button and pause for two seconds
    } else {
      
      next_page()
      Sys.sleep(2)
      
    }
    
  }
  
  # Return only unique values (there might be duplicate entries as the same
  # programme also starts in the next year)
  return(unique(all_programmes))
  
}
```

With this function, we can scrape two list containing programmes related to "data science" or "marketing":

```{r}
scrape_programmes("data science")
```


```{r}
scrape_programmes("marketing")
```

```{r}
scrape_programmes("wine making")
```

Finally, let us close the driver and browser window before closing R:

```{r}
# close the RSelenium processes:
driver$close()
rD$server$stop()

# close the associated Java processes (if using Mac or Linux this may not be necessary -- Google for correct command)
system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
```

#### Concluding remarks

This document is meant to illustrate some common challenges faced when scraping websites with Selenium and focuses on demonstrating functionalities of the package, not on the most efficient approach to scrape the information. As an example, the script first collects the exact number of programmes on the current page to avoid iterating the for-loop over XPaths that do not exist. An easier approach would be to just choose a high number of iterations for each for-loop and combine this with `find$elements()` rather than `find$element()`, because the former returns a list of length zero if no element was found rather than an error (hence it continues running also if a for-loop tries to collect more elements than are displayed on the current page). Another option would be to define an XPath or other selector which matches and selects all programme titles in one go and hence does not require a loop at all. 

There are many further potential extensions to this script. For example, recall that the main function breaks if we search for a term for which the website returns zero hits. Full function testing would go through such cases and build conditionals into the function such that it would not break, but instead e.g. return a list of length zero in this case. Another example is that the `next_page()` function breaks if it is applied on the last page for a given search. The reason is that this last page does not have a "right-arrow" button, so the element does not exist and the code returns an error. To build a script that is robust to such cases would require to either always use `find$elements()` and route to different code parts when the return has length zero, or to catch errors resulting from `find$element()` and then route the code to the alternative part. To build code that does not stop when it encounters errors can be helpful for applications in web scraping and other topics. See for example the following link for the try() function and more advanced approaches: <http://adv-r.had.co.nz/Exceptions-Debugging.html>. Such extensions are left as an exercise here for students who are interested in these topics in more depth.